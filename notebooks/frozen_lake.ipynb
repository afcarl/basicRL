{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "\n",
    "**\"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"**\n",
    "\n",
    "from [openai](https://gym.openai.com/envs/FrozenLake-v0/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the MDP\n",
    "wrapper = gym.Wrapper(gym.make(\"FrozenLake-v0\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MDP has the following form:\n",
    "\n",
    "\n",
    "![frozen](img/fl.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We always can use some methods from the wrapper class to see some aspects of the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Discrete(16)\n",
      "Actions space: Discrete(4)\n",
      "reward range: (-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation space: {}\".format(wrapper.observation_space))\n",
    "print(\"Actions space: {}\".format(wrapper.action_space))\n",
    "print(\"reward range: {}\".format(wrapper.reward_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the method render, we can visualize the agent moving in the enviroment\n",
    "\n",
    "- S is start\n",
    "\n",
    "- F is frozen\n",
    "\n",
    "- H is hole\n",
    "\n",
    "- G is goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "plan = [1,1,1,2]\n",
    "\n",
    "wrapper.reset()\n",
    "wrapper.render()\n",
    "\n",
    "for i in range(len(plan)):\n",
    "        action = plan[i]\n",
    "        obs, reward , done , info = wrapper.step(action)\n",
    "        wrapper.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing 100 episodes with random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward = 0.01\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "\n",
    "episodes = 200\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    wrapper.reset()\n",
    "    while done is False:\n",
    "        action = wrapper.action_space.sample()\n",
    "        _, reward , done , _ = wrapper.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "print(\"Average reward = {}\".format(total_reward/episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can create random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic_policy\n",
    "\n",
    "def create_deterministic_policy(env):\n",
    "    \"\"\"\n",
    "    using an enviroment with discrete states this function returns\n",
    "    a dictionary state:action\n",
    "    \n",
    "    :type env: gym.Env\n",
    "    :rtype: dict {int: int}\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.discrete.Discrete\n",
    "    number_states = env.observation_space.n\n",
    "    number_actions = env.action_space.n \n",
    "    policy = {}\n",
    "    for i in range(number_states):\n",
    "        action = np.random.randint(number_actions,\n",
    "                                   size=1)[0]\n",
    "        assert env.action_space.contains(action)\n",
    "        assert env.observation_space.contains(i)\n",
    "        policy[i]= action\n",
    "    return policy\n",
    "\n",
    "\n",
    "def create_initial_V(env,random=False):\n",
    "    \"\"\"\n",
    "    Using an enviroment with discrete states this function returns\n",
    "    a dictionary state:value. The initial value of an state value function.\n",
    "    It can start with all values set to zeros, or with randon numbers. \n",
    "    \n",
    "    :type env: gym.Env\n",
    "    :type random: boolean\n",
    "    :rtype: np.array\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.discrete.Discrete\n",
    "    number_states = env.observation_space.n\n",
    "    if random:\n",
    "        V = np.random.normal(0, 1, number_states)\n",
    "    else:\n",
    "        V = np.zeros(number_states)\n",
    "    return V\n",
    "\n",
    "def create_initial_Q(env,random=False):\n",
    "    \"\"\"\n",
    "    Using an enviroment with discrete states this function returns\n",
    "    a dictionary state:value. The initial value of an state value function.\n",
    "    It can start with all values set to zeros, or with randon numbers. \n",
    "    \n",
    "    :type env: gym.Env\n",
    "    :type random: boolean\n",
    "    :rtype: np.array\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.discrete.Discrete\n",
    "    number_states = env.observation_space.n\n",
    "    number_actions = env.action_space.n\n",
    "    Q_shape = (number_states, number_actions) \n",
    "    if random:\n",
    "        Q = np.random.normal(0, 1, Q_shape)\n",
    "    else:\n",
    "        Q = np.zeros(Q_shape)\n",
    "    return Q\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each\n",
    "    sets of scores in x.\n",
    "    \n",
    "    :type x: np.array\n",
    "    :rtype: np.array\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) \n",
    "\n",
    "def create_stochastic_policy(env,low=5.0,high=10.0):\n",
    "    \"\"\"\n",
    "    using an enviroment with discrete states this function returns\n",
    "    a dictionary state:[prob of actions]\n",
    "    \n",
    "    :type env: gym.Env\n",
    "    :rtype: dict {int: [float]}\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.discrete.Discrete\n",
    "    number_states = env.observation_space.n\n",
    "    number_actions = env.action_space.n \n",
    "    policy = {}\n",
    "    for i in range(number_states):\n",
    "        actions = np.random.randint(low,\n",
    "                                    high,\n",
    "                                    size=number_actions)\n",
    "        actions = softmax(actions)\n",
    "        assert env.observation_space.contains(i)\n",
    "        policy[i]= actions\n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking an deterministic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward = 0.0\n",
      "Policy = \n",
      "{0: 1, 1: 2, 2: 0, 3: 3, 4: 3, 5: 0, 6: 3, 7: 1, 8: 1, 9: 0, 10: 2, 11: 2, 12: 0, 13: 1, 14: 1, 15: 1}\n"
     ]
    }
   ],
   "source": [
    "policy = create_deterministic_policy(wrapper)\n",
    "\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "episodes = 200\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    obs = wrapper.reset()\n",
    "    while done is False:\n",
    "        action = policy[obs]\n",
    "        obs, reward , done , _ = wrapper.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "print(\"Average reward = {}\".format(total_reward/episodes))\n",
    "print(\"Policy = \")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking an stochastic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward = 0.005\n",
      "Policy = \n",
      "  0 [0.0963, 0.7112, 0.0963, 0.0963]\n",
      "  1 [0.0174, 0.0174, 0.9479, 0.0174]\n",
      "  2 [0.0339, 0.6815, 0.2507, 0.0339]\n",
      "  3 [0.5344, 0.0723, 0.1966, 0.1966]\n",
      "  4 [0.0723, 0.1966, 0.5344, 0.1966]\n",
      "  5 [0.0128, 0.0347, 0.6964, 0.2562]\n",
      "  6 [0.2500, 0.2500, 0.2500, 0.2500]\n",
      "  7 [0.0169, 0.0458, 0.9205, 0.0169]\n",
      "  8 [0.0414, 0.8310, 0.1125, 0.0152]\n",
      "  9 [0.0237, 0.4763, 0.0237, 0.4763]\n",
      " 10 [0.4643, 0.0085, 0.0628, 0.4643]\n",
      " 11 [0.0237, 0.4763, 0.4763, 0.0237]\n",
      " 12 [0.0403, 0.1096, 0.8098, 0.0403]\n",
      " 13 [0.1096, 0.0403, 0.8098, 0.0403]\n",
      " 14 [0.4576, 0.0619, 0.0228, 0.4576]\n",
      " 15 [0.2500, 0.2500, 0.2500, 0.2500]\n"
     ]
    }
   ],
   "source": [
    "policy = create_stochastic_policy(wrapper)\n",
    "\n",
    "actions = list(range(wrapper.action_space.n))\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "episodes = 200\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    obs = wrapper.reset()\n",
    "    while done is False:\n",
    "        probabilities = policy[obs]\n",
    "        action = np.random.choice(actions, 1, p=probabilities)[0]\n",
    "        obs, reward , done , _ = wrapper.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "print(\"Average reward = {}\".format(total_reward/episodes))\n",
    "print(\"Policy = \")\n",
    "for state, actions in policy.items():\n",
    "    actions = \", \".join([\"{:.4f}\".format(prob) for prob in actions])\n",
    "    print(\"{0:3d} [{1}]\".format(state, actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte carlo estimation\n",
    "\n",
    "**Monte Carlo methods** are ways of solving the reinforcement learning problem based on averaging sample returns.\n",
    "\n",
    "Suppose we wish to estimate $V^{\\pi}(s)$, the value of state $s$ under policy $\\pi$, given a set of episodes obtained by following $\\pi$ and passing through $s$. Each occurence of state $s$ in an episode is called a **visit** to $s$. We can also estimate $Q^{\\pi}(s,a)$\n",
    "\n",
    "- ** First-visit MC method**, $V^{\\pi}(s)$ and $Q^{\\pi}(s,a)$ are estimated as the first return from a visits to $s$ (taking action $a$) in a set of episodes\n",
    "\n",
    "- ** Every-visit MC method**, $V^{\\pi}(s)$ and $Q^{\\pi}(s,a)$ are estimated as the average of the returns following all the visits to $s$ (taking action $a$) in a set of episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def first_visit_MC(env, policy, episodes, deterministic=True, random=False):\n",
    "#     \"\"\"\n",
    "#     Calculates V and Q values for the policy in a set of episodes\n",
    "\n",
    "#     :type env: gym.Env\n",
    "#     :type policy: dict {int: int} or {int: [float]}\n",
    "#     :type episodes: int\n",
    "#     :type deterministic: boolean\n",
    "#     :type random: boolean\n",
    "#     :rtype V: np.array, shape = env..observation_space.n\n",
    "#     :rtype Q: np.array, shape = (env.observation_space.n,\n",
    "#                                  env.action_space.n)\n",
    "#     \"\"\"\n",
    "#     actions = list(range(env.action_space.n))\n",
    "#     V = create_initial_V(env, random)\n",
    "#     Q = create_initial_Q(env, random)\n",
    "#     for i in range(episodes):\n",
    "#         Q_prime = defaultdict(lambda:None)\n",
    "#         V_prime = defaultdict(lambda:None)\n",
    "#         old_obs = env.reset()\n",
    "#         done = False\n",
    "#         R = 0\n",
    "#         while not done:\n",
    "#             if deterministic:\n",
    "#                 action = policy[old_obs]\n",
    "#             else:\n",
    "#                 probabilities = policy[old_obs]\n",
    "#                 action = np.random.choice(actions, 1, p=probabilities)[0]\n",
    "#             obs, reward , done , _ = wrapper.step(action)\n",
    "#             R += reward\n",
    "#             if Q_prime[(old_obs, action)] is None:\n",
    "#                 Q_prime[(old_obs, action)] = R\n",
    "#             if V_prime[old_obs] is None:\n",
    "#                 V_prime[old_obs] = R\n",
    "#             old_obs = obs\n",
    "#         for state, action in Q_prime:\n",
    "#             Q[state][action] += R - Q_prime[(state, action)]\n",
    "#         for state in V_prime:\n",
    "#             V[state] += R - V_prime[state]\n",
    "            \n",
    "#     Q = Q / episodes\n",
    "#     V = V / episodes\n",
    "#     return V, Q\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def every_visit_MC(env, policy, episodes, deterministic=True, random=False):\n",
    "    \"\"\"\n",
    "    Calculates V and Q values for the policy in a set of episodes\n",
    "\n",
    "    :type env: gym.Env\n",
    "    :type policy: dict {int: int} or {int: [float]}\n",
    "    :type episodes: int\n",
    "    :type deterministic: boolean\n",
    "    :type random: boolean\n",
    "    :rtype V: np.array, shape = env..observation_space.n\n",
    "    :rtype Q: np.array, shape = (env.observation_space.n,\n",
    "                                 env.action_space.n)\n",
    "    \"\"\"\n",
    "    actions = list(range(env.action_space.n))\n",
    "    V = create_initial_V(env, random)\n",
    "    Q = create_initial_Q(env, random)\n",
    "    for i in range(episodes):\n",
    "        Q_prime = defaultdict(lambda:[])\n",
    "        V_prime = defaultdict(lambda:[])\n",
    "        old_obs = env.reset()\n",
    "        done = False\n",
    "        R = 0\n",
    "        while not done:\n",
    "            if deterministic:\n",
    "                action = policy[old_obs]\n",
    "            else:\n",
    "                probabilities = policy[old_obs]\n",
    "                action = np.random.choice(actions, 1, p=probabilities)[0]\n",
    "            obs, reward , done , _ = wrapper.step(action)\n",
    "            R += reward\n",
    "            Q_prime[(old_obs, action)].append(R)          \n",
    "            V_prime[old_obs].append(R)\n",
    "            old_obs = obs\n",
    "        for state, action in Q_prime:\n",
    "            Q[state][action] += R - np.mean(Q_prime[(state, action)])\n",
    "        for state in V_prime:\n",
    "            V[state] += R - np.mean(V_prime[state])\n",
    "\n",
    "    Q = Q / episodes\n",
    "    V = V / episodes\n",
    "    return V, Q\n",
    "    \n",
    "def first_visit_MC(env, policy, episodes, deterministic=True, random=False):\n",
    "    \"\"\"\n",
    "    Calculates V and Q values for the policy in a set of episodes\n",
    "\n",
    "    :type env: gym.Env\n",
    "    :type policy: dict {int: int} or {int: [float]}\n",
    "    :type episodes: int\n",
    "    :type deterministic: boolean\n",
    "    :type random: boolean\n",
    "    :rtype V: np.array, shape = env.observation_space.n\n",
    "    :rtype Q: np.array, shape = (env.observation_space.n,\n",
    "                                 env.action_space.n)\n",
    "    \"\"\"\n",
    "    value_deb1 = []\n",
    "    value_deb2 = []\n",
    "    state_count = np.ones(env.observation_space.n)\n",
    "    actions = list(range(env.action_space.n))\n",
    "    V = create_initial_V(env, random)\n",
    "    Q = create_initial_Q(env, random)\n",
    "    for i in range(episodes):\n",
    "        Q_prime = defaultdict(lambda:None)\n",
    "        V_prime = defaultdict(lambda:None)\n",
    "        old_obs = env.reset()\n",
    "        done = False\n",
    "        R = 0\n",
    "        while not done:\n",
    "            state_count[old_obs] += 1\n",
    "            if deterministic:\n",
    "                action = policy[old_obs]\n",
    "            else:\n",
    "                probabilities = policy[old_obs]\n",
    "                action = np.random.choice(actions,\n",
    "                                          p=probabilities)\n",
    "            obs, reward , done , _ = env.step(action)\n",
    "            if Q_prime[(old_obs, action)] is None:\n",
    "                Q_prime[(old_obs, action)] = R\n",
    "            if V_prime[old_obs] is None:\n",
    "                V_prime[old_obs] = R\n",
    "            R += reward\n",
    "            old_obs = obs\n",
    "        for state, action in Q_prime:\n",
    "            Q[state][action] += R - Q_prime[(state, action)]\n",
    "        for state in V_prime:\n",
    "            V[state] += R - V_prime[state]\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            value_deb1.append(V[14]/state_count[14])\n",
    "            value_deb2.append(V[0]/state_count[0])\n",
    "    \n",
    "    Q = Q / episodes\n",
    "    V = V / state_count\n",
    "    return V, Q, value_deb1, value_deb2\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating V and Q with first visit MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = create_stochastic_policy(wrapper)\n",
    "\n",
    "episodes = 20000\n",
    "\n",
    "V_pi, Q_pi, _, _ = first_visit_MC(wrapper, pi, episodes, deterministic=False, random=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating V and Q with every visit MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pi = create_stochastic_policy(wrapper)\n",
    "\n",
    "# episodes = 20000\n",
    "\n",
    "# V_pi, Q_pi = every_visit_MC(wrapper, pi, episodes, deterministic=False, random=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**after this processes we can visualize the V as a matrix and the Q table**\n",
    "\n",
    "\n",
    "![frozen](img/V_Q.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAFTCAYAAAC3cPB9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJTQRcQ0lNBFxjf8HaDwCXGtG1fwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e86cfed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax = sns.heatmap(V_pi.reshape((4,4)), annot=True)\n",
    "ax.grid()\n",
    "fig.suptitle(\"V matrix\", fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "# ax = sns.heatmap(Q_pi, annot=True)\n",
    "# fig.suptitle(\"Q table\", fontsize=18, fontweight='bold')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
