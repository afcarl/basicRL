{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "\n",
    "**\"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"**\n",
    "\n",
    "from [openai](https://gym.openai.com/envs/FrozenLake-v0/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the MDP\n",
    "wrapper = gym.Wrapper(gym.make(\"FrozenLake-v0\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MDP has the following form:\n",
    "\n",
    "\n",
    "![frozen](img/fl.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We always can use some methods from the wrapper class to see some aspects of the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Discrete(16)\n",
      "Actions space: Discrete(4)\n",
      "reward range: (-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation space: {}\".format(wrapper.observation_space))\n",
    "print(\"Actions space: {}\".format(wrapper.action_space))\n",
    "print(\"reward range: {}\".format(wrapper.reward_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the method render, we can visualize the agent moving in the enviroment\n",
    "\n",
    "- S is start\n",
    "\n",
    "- F is frozen\n",
    "\n",
    "- H is hole\n",
    "\n",
    "- G is goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "plan = [1,1,1,2]\n",
    "\n",
    "wrapper.reset()\n",
    "wrapper.render()\n",
    "\n",
    "for i in range(len(plan)):\n",
    "        action = plan[i]\n",
    "        obs, reward , done , info = wrapper.step(action)\n",
    "        wrapper.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing 100 episodes with random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward = 0.01\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "\n",
    "episodes = 200\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    wrapper.reset()\n",
    "    while done is False:\n",
    "        action = wrapper.action_space.sample()\n",
    "        _, reward , done , _ = wrapper.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "print(\"Average reward = {}\".format(total_reward/episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can create random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_deterministic_policy(env):\n",
    "    \"\"\"\n",
    "    using an enviroment with discrete states this function returns\n",
    "    a dictionary state:action\n",
    "    \n",
    "    :type env: gym.Env\n",
    "    :rtype: dict {int: int}\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.discrete.Discrete\n",
    "    number_states = env.observation_space.n\n",
    "    number_actions = env.action_space.n \n",
    "    policy = {}\n",
    "    for i in range(number_states):\n",
    "        action = np.random.randint(number_actions,\n",
    "                                   size=1)[0]\n",
    "        assert env.action_space.contains(action)\n",
    "        assert env.observation_space.contains(i)\n",
    "        policy[i]= action\n",
    "    return policy\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each\n",
    "    sets of scores in x.\n",
    "    \n",
    "    :type x: np.array\n",
    "    :rtype: np.array\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) \n",
    "\n",
    "def creat_stochastic_policy(env,low=5.0,high=10.0):\n",
    "    \"\"\"\n",
    "    using an enviroment with discrete states this function returns\n",
    "    a dictionary state:[prob of actions]\n",
    "    \n",
    "    :type env: gym.Env\n",
    "    :rtype: dict {int: [float]}\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.discrete.Discrete\n",
    "    number_states = env.observation_space.n\n",
    "    number_actions = env.action_space.n \n",
    "    policy = {}\n",
    "    for i in range(number_states):\n",
    "        actions = np.random.randint(low,\n",
    "                                    high,\n",
    "                                    size=number_actions)\n",
    "        actions = softmax(actions)\n",
    "        assert env.observation_space.contains(i)\n",
    "        policy[i]= actions\n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking an deterministic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward = 0.035\n",
      "Policy = \n",
      "{0: 3, 1: 2, 2: 2, 3: 0, 4: 3, 5: 0, 6: 0, 7: 1, 8: 0, 9: 1, 10: 2, 11: 3, 12: 0, 13: 2, 14: 3, 15: 1}\n"
     ]
    }
   ],
   "source": [
    "policy = creat_deterministic_policy(wrapper)\n",
    "\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "episodes = 200\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    obs = wrapper.reset()\n",
    "    while done is False:\n",
    "        action = policy[obs]\n",
    "        obs, reward , done , _ = wrapper.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "print(\"Average reward = {}\".format(total_reward/episodes))\n",
    "print(\"Policy = \")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking an stochastic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward = 0.035\n",
      "Policy = \n",
      "0 [ 0.08259454  0.08259454  0.61029569  0.22451524]\n",
      "1 [ 0.08714432  0.64391426  0.0320586   0.23688282]\n",
      "2 [ 0.22451524  0.61029569  0.08259454  0.08259454]\n",
      "3 [ 0.47628706  0.47628706  0.02371294  0.02371294]\n",
      "4 [ 0.25618664  0.01275478  0.69638749  0.03467109]\n",
      "5 [ 0.03467109  0.25618664  0.01275478  0.69638749]\n",
      "6 [ 0.06193488  0.45764028  0.02278457  0.45764028]\n",
      "7 [ 0.69638749  0.01275478  0.25618664  0.03467109]\n",
      "8 [ 0.3994863   0.3994863   0.1469628   0.05406459]\n",
      "9 [ 0.01736167  0.94791499  0.01736167  0.01736167]\n",
      "10 [ 0.01275478  0.25618664  0.69638749  0.03467109]\n",
      "11 [ 0.64391426  0.08714432  0.23688282  0.0320586 ]\n",
      "12 [ 0.09625514  0.71123459  0.09625514  0.09625514]\n",
      "13 [ 0.3994863   0.3994863   0.1469628   0.05406459]\n",
      "14 [ 0.02278457  0.45764028  0.06193488  0.45764028]\n",
      "15 [ 0.44039854  0.05960146  0.05960146  0.44039854]\n"
     ]
    }
   ],
   "source": [
    "policy = creat_stochastic_policy(wrapper)\n",
    "\n",
    "actions = list(range(wrapper.action_space.n))\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "episodes = 200\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    obs = wrapper.reset()\n",
    "    while done is False:\n",
    "        probabilities = policy[obs]\n",
    "        action = np.random.choice(actions, 1, p=probabilities)[0]\n",
    "        obs, reward , done , _ = wrapper.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "print(\"Average reward = {}\".format(total_reward/episodes))\n",
    "print(\"Policy = \")\n",
    "for key in list(policy.keys()):\n",
    "    print(key, policy[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
